{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15853,"status":"ok","timestamp":1673581547422,"user":{"displayName":"Aditya IIT Mandi","userId":"15697490575660665161"},"user_tz":-330},"id":"K8ssChTIcbeD","outputId":"19265bad-bf80-4a02-979a-82707f9d8d27"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/tensorflow/docs\n","  Cloning https://github.com/tensorflow/docs to /tmp/pip-req-build-te1_z1eq\n","  Running command git clone --filter=blob:none --quiet https://github.com/tensorflow/docs /tmp/pip-req-build-te1_z1eq\n","  Resolved https://github.com/tensorflow/docs to commit f743bbc0645dbdec6a92dbe7f3768a60f109e87d\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: astor in /usr/local/lib/python3.8/dist-packages (from tensorflow-docs==0.0.0.dev0) (0.8.1)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from tensorflow-docs==0.0.0.dev0) (1.3.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-docs==0.0.0.dev0) (2.11.3)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.8/dist-packages (from tensorflow-docs==0.0.0.dev0) (5.7.1)\n","Requirement already satisfied: protobuf<3.20,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-docs==0.0.0.dev0) (3.19.6)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from tensorflow-docs==0.0.0.dev0) (6.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->tensorflow-docs==0.0.0.dev0) (2.0.1)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.8/dist-packages (from nbformat->tensorflow-docs==0.0.0.dev0) (5.1.2)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.8/dist-packages (from nbformat->tensorflow-docs==0.0.0.dev0) (4.3.3)\n","Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.8/dist-packages (from nbformat->tensorflow-docs==0.0.0.dev0) (5.7.1)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.8/dist-packages (from nbformat->tensorflow-docs==0.0.0.dev0) (2.16.2)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->tensorflow-docs==0.0.0.dev0) (22.2.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->tensorflow-docs==0.0.0.dev0) (0.19.3)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->tensorflow-docs==0.0.0.dev0) (5.10.2)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.8/dist-packages (from jupyter-core->nbformat->tensorflow-docs==0.0.0.dev0) (2.6.2)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat->tensorflow-docs==0.0.0.dev0) (3.11.0)\n","Building wheels for collected packages: tensorflow-docs\n","  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tensorflow-docs: filename=tensorflow_docs-0.0.0.dev0-py3-none-any.whl size=184468 sha256=1740fce9a227008f61143aacab468ee4e64c09330d1bc3ec3576ca47f97e62dc\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-s5atl3lm/wheels/3b/ee/a2/ab4d36a9a4af495bcb936f3e849d4b497b65fa40548a68d6c3\n","Successfully built tensorflow-docs\n","Installing collected packages: tensorflow-docs\n","Successfully installed tensorflow-docs-0.0.0.dev0\n"]}],"source":["# Importing libraries\n","!pip install git+https://github.com/tensorflow/docs\n","from absl import logging\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","from tensorflow_docs.vis import embed\n","import random\n","import re\n","import os\n","import tempfile\n","import ssl\n","import cv2\n","import numpy as np\n","import imageio\n","from IPython import display\n","from urllib import request\n","import re\n","import pprint\n","import glob\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30774,"status":"ok","timestamp":1673581582258,"user":{"displayName":"Aditya IIT Mandi","userId":"15697490575660665161"},"user_tz":-330},"id":"y4PCq8PDcyP-","outputId":"0ee9a112-6a67-466b-ddc6-57844bdf08cd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# Mounting google drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KpaUKuFoLUbQ"},"outputs":[],"source":["# Get the kinetics-600 action labels from the GitHub repository.\n","\n","with open('/content/drive/MyDrive/Scomp/Visual/Dataset/labels.txt') as f:\n","    l = f.readlines()\n","    labels=len(l)\n","\n","with open('/content/drive/MyDrive/Scomp/Visual/Dataset/labels.txt','r') as f_in:\n","    lines = f_in.read()\n","    list=re.split('; |, |\\*|\\n',lines)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FLYKcP_MI0cX"},"outputs":[],"source":["def crop_center_square(frame):\n","  y, x = frame.shape[0:2]\n","  min_dim = min(y, x)\n","  start_x = (x // 2) - (min_dim // 2)\n","  start_y = (y // 2) - (min_dim // 2)\n","  return frame[start_y:start_y+min_dim,start_x:start_x+min_dim]\n","\n","def load_video(path, max_frames=0, resize=(224, 224)):\n","  cap = cv2.VideoCapture(path)\n","  frames = []\n","  try:\n","    while True:\n","      ret, frame = cap.read()\n","      if not ret:\n","        break\n","      frame = crop_center_square(frame)\n","      frame = cv2.resize(frame, resize)\n","      frame = frame[:, :, [2, 1, 0]]\n","      frames.append(frame)\n","\n","      if len(frames) == max_frames:\n","        break\n","  finally:\n","    cap.release()\n","  return np.array(frames) / 255.0\n","def to_gif(images):\n","  converted_images = np.clip(images * 255, 0, 255).astype(np.uint8)\n","  imageio.mimsave('./animation.gif', converted_images, fps=25)\n","  return embed.embed_file('./animation.gif')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2656554,"status":"ok","timestamp":1673558061073,"user":{"displayName":"Aditya IIT Mandi","userId":"15697490575660665161"},"user_tz":-330},"id":"HC9LKxMnOiac","outputId":"590505a2-d274-4626-9916-687e3fa3fcea"},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 230s 230s/step\n","Top 2 actions:\n","  staring               : 24.57%\n","  crossing eyes         : 15.81%\n","1/1 [==============================] - 133s 133s/step\n","Top 2 actions:\n","  headbanging           : 11.75%\n","  hugging (not baby)    : 9.23%\n","1/1 [==============================] - 211s 211s/step\n","Top 2 actions:\n","  staring               : 36.18%\n","  photobombing          : 13.38%\n","1/1 [==============================] - 212s 212s/step\n","Top 2 actions:\n","  staring               : 29.08%\n","  crossing eyes         : 8.45%\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0b88cb1af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 228s 228s/step\n","Top 2 actions:\n","  news anchoring        : 15.30%\n","  staring               : 13.15%\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0b76265940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 219s 219s/step\n","Top 2 actions:\n","  crossing eyes         : 17.21%\n","  winking               : 11.08%\n","1/1 [==============================] - 221s 221s/step\n","Top 2 actions:\n","  staring               : 30.96%\n","  crossing eyes         : 16.77%\n","1/1 [==============================] - 219s 219s/step\n","Top 2 actions:\n","  massaging person's head: 66.89%\n","  raising eyebrows      : 3.78%\n","1/1 [==============================] - 219s 219s/step\n","Top 2 actions:\n","  staring               : 17.42%\n","  laughing              : 12.67%\n","1/1 [==============================] - 212s 212s/step\n","Top 2 actions:\n","  staring               : 49.64%\n","  crossing eyes         : 27.80%\n"]}],"source":["# HAPPY\n","\n","path = \"/content/drive/MyDrive/Scomp/Visual/Dataset/Youtube Data/Happy\"\n","wav_files = glob.glob(path + \"/*.mp4\")\n","for i in wav_files:\n","    video_path=i\n","    sample_video = load_video(video_path)[:100]\n","    sample_video.shape\n","\n","    hub_url = \"https://tfhub.dev/tensorflow/movinet/a2/base/kinetics-600/classification/3\"\n","\n","    encoder = hub.KerasLayer(hub_url, trainable=True)\n","\n","    inputs = tf.keras.layers.Input(\n","        shape=[None,None,None,3],\n","        dtype=tf.float32,\n","        name='image')\n","\n","    outputs = encoder(dict(image=inputs))\n","\n","    model = tf.keras.Model(inputs, outputs, name='movinet')\n","\n","    def predict(sample_video):\n","      # Add a batch axis to the sample video.\n","      model_input = tf.constant(sample_video, dtype=tf.float32)[tf.newaxis, ...]\n","\n","      logits= model.predict(model_input)\n","      logits=logits[0]\n","      probabilities = tf.nn.softmax(logits)\n","\n","      print(\"Top 2 actions:\")\n","      for i in np.argsort(probabilities)[::-1][:2]:\n","        print(f\"  {list[i]:22}: {probabilities[i] * 100:2.2f}%\")  \n","    result=predict(sample_video) "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2696783,"status":"ok","timestamp":1673563541622,"user":{"displayName":"Aditya IIT Mandi","userId":"15697490575660665161"},"user_tz":-330},"id":"nzIpTk2XZPxY","outputId":"386957a7-08b7-442f-ceda-c14ff9cd108e"},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 221s 221s/step\n","Top 2 actions:\n","  crying                : 26.69%\n","  kissing               : 12.33%\n","1/1 [==============================] - 216s 216s/step\n","Top 2 actions:\n","  kissing               : 13.80%\n","  crying                : 8.65%\n","1/1 [==============================] - 219s 219s/step\n","Top 2 actions:\n","  opening door          : 9.15%\n","  pull ups              : 5.59%\n","1/1 [==============================] - 215s 215s/step\n","Top 2 actions:\n","  acting in play        : 56.07%\n","  singing               : 8.23%\n","1/1 [==============================] - 215s 215s/step\n","Top 2 actions:\n","  crying                : 10.61%\n","  sneezing              : 10.43%\n","1/1 [==============================] - 216s 216s/step\n","Top 2 actions:\n","  answering questions   : 15.65%\n","  using inhaler         : 14.75%\n","1/1 [==============================] - 215s 215s/step\n","Top 2 actions:\n","  putting on sari       : 23.39%\n","  celebrating           : 11.41%\n","1/1 [==============================] - 218s 218s/step\n","Top 2 actions:\n","  staring               : 48.31%\n","  crying                : 29.85%\n","1/1 [==============================] - 217s 217s/step\n","Top 2 actions:\n","  staring               : 20.45%\n","  crying                : 12.89%\n","1/1 [==============================] - 217s 217s/step\n","Top 2 actions:\n","  giving or receiving award: 21.16%\n","  opening bottle (not wine): 8.24%\n"]}],"source":["# SAD\n","\n","path = \"/content/drive/MyDrive/Scomp/Visual/Dataset/Youtube Data/Sad\"\n","wav_files = glob.glob(path + \"/*.mp4\")\n","\n","for i in wav_files:\n","    video_path=i\n","    sample_video = load_video(video_path)[:100]\n","    sample_video.shape\n","\n","    hub_url = \"https://tfhub.dev/tensorflow/movinet/a2/base/kinetics-600/classification/3\"\n","\n","    encoder = hub.KerasLayer(hub_url, trainable=True)\n","\n","    inputs = tf.keras.layers.Input(\n","        shape=[None,None,None,3],\n","        dtype=tf.float32,\n","        name='image')\n","\n","    outputs = encoder(dict(image=inputs))\n","\n","    model = tf.keras.Model(inputs, outputs, name='movinet')\n","\n","    def predict(sample_video):\n","      # Add a batch axis to the sample video.\n","      model_input = tf.constant(sample_video, dtype=tf.float32)[tf.newaxis, ...]\n","\n","      logits= model.predict(model_input)\n","      logits=logits[0]\n","      probabilities = tf.nn.softmax(logits)\n","\n","      print(\"Top 2 actions:\")\n","      for i in np.argsort(probabilities)[::-1][:2]:\n","        print(f\"  {list[i]:22}: {probabilities[i] * 100:2.2f}%\")\n","  \n","    result=predict(sample_video) "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2649692,"status":"ok","timestamp":1673560806785,"user":{"displayName":"Aditya IIT Mandi","userId":"15697490575660665161"},"user_tz":-330},"id":"DVjypUVTZSRA","outputId":"37c171ab-6037-4f57-c8e1-6eb499c382f3"},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 215s 215s/step\n","Top 2 actions:\n","  sleeping              : 16.07%\n","  slapping              : 14.12%\n","1/1 [==============================] - 218s 218s/step\n","Top 2 actions:\n","  wrapping present      : 26.13%\n","  opening present       : 10.17%\n","1/1 [==============================] - 219s 219s/step\n","Top 2 actions:\n","  moving furniture      : 21.38%\n","  making the bed        : 12.83%\n","1/1 [==============================] - 213s 213s/step\n","Top 2 actions:\n","  sneezing              : 38.85%\n","  burping               : 28.97%\n","1/1 [==============================] - 221s 221s/step\n","Top 2 actions:\n","  photobombing          : 11.85%\n","  singing               : 8.49%\n","1/1 [==============================] - 217s 217s/step\n","Top 2 actions:\n","  opening door          : 88.57%\n","  opening refrigerator  : 2.08%\n","1/1 [==============================] - 222s 222s/step\n","Top 2 actions:\n","  sneezing              : 9.95%\n","  crying                : 8.85%\n","1/1 [==============================] - 218s 218s/step\n","Top 2 actions:\n","  opening door          : 72.72%\n","  opening refrigerator  : 2.67%\n","1/1 [==============================] - 214s 214s/step\n","Top 2 actions:\n","  acting in play        : 20.19%\n","  singing               : 14.58%\n","1/1 [==============================] - 221s 221s/step\n","Top 2 actions:\n","  tossing coin          : 15.51%\n","  opening door          : 8.71%\n"]}],"source":["# SURPRISE\n","\n","path = \"/content/drive/MyDrive/Scomp/Visual/Dataset/Youtube Data/Surprise\"\n","wav_files = glob.glob(path + \"/*.mp4\")\n","\n","for i in wav_files:\n","    video_path=i\n","    sample_video = load_video(video_path)[:100]\n","    sample_video.shape\n","\n","    hub_url = \"https://tfhub.dev/tensorflow/movinet/a2/base/kinetics-600/classification/3\"\n","\n","    encoder = hub.KerasLayer(hub_url, trainable=True)\n","\n","    inputs = tf.keras.layers.Input(\n","        shape=[None,None,None,3],\n","        dtype=tf.float32,\n","        name='image')\n","\n","    outputs = encoder(dict(image=inputs))\n","\n","    model = tf.keras.Model(inputs, outputs, name='movinet')\n","\n","    def predict(sample_video):\n","      # Add a batch axis to the sample video.\n","      model_input = tf.constant(sample_video, dtype=tf.float32)[tf.newaxis, ...]\n","\n","      logits= model.predict(model_input)\n","      logits=logits[0]\n","      probabilities = tf.nn.softmax(logits)\n","\n","      print(\"Top 2 actions:\")\n","      for i in np.argsort(probabilities)[::-1][:2]:\n","        print(f\"  {list[i]:22}: {probabilities[i] * 100:2.2f}%\")\n","    \n","    predict(sample_video) "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"dZR8N_IzZQ5b","outputId":"05f204de-ef33-405d-932b-8f75b2e46e8b"},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 224s 224s/step\n","Top 2 actions:\n","  yawning               : 50.46%\n","  sneezing              : 15.21%\n","1/1 [==============================] - 219s 219s/step\n","Top 2 actions:\n","  sneezing              : 15.53%\n","  cracking neck         : 8.06%\n","1/1 [==============================] - 214s 214s/step\n","Top 2 actions:\n","  playing paintball     : 10.38%\n","  catching fish         : 3.80%\n","1/1 [==============================] - 219s 219s/step\n","Top 2 actions:\n","  smoking               : 12.70%\n","  sleeping              : 6.73%\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f112a9245e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 215s 215s/step\n","Top 2 actions:\n","  crying                : 18.84%\n","  burping               : 8.66%\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f11176cf040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 226s 226s/step\n","Top 2 actions:\n","  motorcycling          : 45.77%\n","  riding scooter        : 8.88%\n","1/1 [==============================] - 212s 212s/step\n","Top 2 actions:\n","  staring               : 22.07%\n","  whistling             : 12.23%\n","1/1 [==============================] - 208s 208s/step\n","Top 2 actions:\n","  smoking               : 31.38%\n","  testifying            : 16.76%\n","1/1 [==============================] - 215s 215s/step\n","Top 2 actions:\n","  staring               : 47.72%\n","  crying                : 13.78%\n","1/1 [==============================] - 207s 207s/step\n","Top 2 actions:\n","  trimming or shaving beard: 51.96%\n","  tasting beer          : 13.31%\n"]}],"source":["# ANGER\n","\n","path = \"/content/drive/MyDrive/Scomp/Visual/Dataset/Youtube Data/Anger\"\n","wav_files = glob.glob(path + \"/*.mp4\")\n","\n","for i in wav_files:\n","    video_path=i\n","    sample_video = load_video(video_path)[:100]\n","    sample_video.shape\n","\n","    hub_url = \"https://tfhub.dev/tensorflow/movinet/a2/base/kinetics-600/classification/3\"\n","\n","    encoder = hub.KerasLayer(hub_url, trainable=True)\n","\n","    inputs = tf.keras.layers.Input(\n","        shape=[None,None,None,3],\n","        dtype=tf.float32,\n","        name='image')\n","\n","    outputs = encoder(dict(image=inputs))\n","\n","    model = tf.keras.Model(inputs, outputs, name='movinet')\n","\n","    def predict(sample_video):\n","      # Add a batch axis to the sample video.\n","      model_input = tf.constant(sample_video, dtype=tf.float32)[tf.newaxis, ...]\n","\n","      logits= model.predict(model_input)\n","      logits=logits[0]\n","      probabilities = tf.nn.softmax(logits)\n","\n","      print(\"Top 2 actions:\")\n","      for i in np.argsort(probabilities)[::-1][:2]:\n","        print(f\"  {list[i]:22}: {probabilities[i] * 100:2.2f}%\")\n","   \n","    predict(sample_video) "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2401579,"status":"ok","timestamp":1673552118700,"user":{"displayName":"Aditya IIT Mandi","userId":"15697490575660665161"},"user_tz":-330},"id":"ChOU0SKxZRnT","outputId":"1e33fa0d-f16a-4db0-c6f2-6f21258fca98"},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 213s 213s/step\n","Top 2 actions:\n","  news anchoring        : 56.27%\n","  answering questions   : 6.49%\n","1/1 [==============================] - 211s 211s/step\n","Top 2 actions:\n","  staring               : 19.67%\n","  finger snapping       : 13.37%\n","1/1 [==============================] - 208s 208s/step\n","Top 2 actions:\n","  massaging neck        : 17.56%\n","  staring               : 14.10%\n","1/1 [==============================] - 203s 203s/step\n","Top 2 actions:\n","  staring               : 19.39%\n","  singing               : 13.24%\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fdddcbc2160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 127s 127s/step\n","Top 2 actions:\n","  testifying            : 28.37%\n","  news anchoring        : 19.24%\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fdde20943a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 203s 203s/step\n","Top 2 actions:\n","  news anchoring        : 49.74%\n","  presenting weather forecast: 33.60%\n","1/1 [==============================] - 202s 202s/step\n","Top 2 actions:\n","  waving hand           : 16.05%\n","  slapping              : 15.16%\n","1/1 [==============================] - 228s 228s/step\n","Top 2 actions:\n","  testifying            : 61.02%\n","  answering questions   : 16.07%\n","1/1 [==============================] - 208s 208s/step\n","Top 2 actions:\n","  news anchoring        : 88.49%\n","  presenting weather forecast: 5.36%\n","1/1 [==============================] - 215s 215s/step\n","Top 2 actions:\n","  news anchoring        : 14.35%\n","  testifying            : 11.03%\n"]}],"source":["# NEUTRAL\n","\n","path = \"/content/drive/MyDrive/Scomp/Visual/Dataset/Youtube Data/Neutral\"\n","wav_files = glob.glob(path + \"/*.mp4\")\n","\n","for i in wav_files:\n","    video_path=i\n","    sample_video = load_video(video_path)[:100]\n","    sample_video.shape\n","\n","    hub_url = \"https://tfhub.dev/tensorflow/movinet/a2/base/kinetics-600/classification/3\"\n","\n","    encoder = hub.KerasLayer(hub_url, trainable=True)\n","\n","    inputs = tf.keras.layers.Input(\n","        shape=[None,None,None,3],\n","        dtype=tf.float32,\n","        name='image')\n","\n","    outputs = encoder(dict(image=inputs))\n","\n","    model = tf.keras.Model(inputs, outputs, name='movinet')\n","\n","    def predict(sample_video):\n","      # Add a batch axis to the sample video.\n","      model_input = tf.constant(sample_video, dtype=tf.float32)[tf.newaxis, ...]\n","\n","      logits= model.predict(model_input)\n","      logits=logits[0]\n","      probabilities = tf.nn.softmax(logits)\n","\n","      print(\"Top 2 actions:\")\n","      for i in np.argsort(probabilities)[::-1][:2]:\n","        print(f\"  {list[i]:22}: {probabilities[i] * 100:2.2f}%\")\n","    \n","    predict(sample_video) "]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}